{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33da5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import openpyxl\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "\n",
    "needed = [\n",
    "    (\"tokenizers/punkt\", \"punkt\"),\n",
    "    (\"tokenizers/punkt_tab\", \"punkt_tab\"),   # <-- thêm cái này\n",
    "    (\"corpora/stopwords\", \"stopwords\"),\n",
    "]\n",
    "\n",
    "for path, pkg in needed:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf629d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df type: <class 'pandas.core.frame.DataFrame'>\n",
      "columns: ['Tweet', 'Final Votes']\n",
      "Data after cleaning:\n",
      "                                               Tweet  \\\n",
      "0  RT @BirdGang316: If you a bird throw it up #bi...   \n",
      "1  RT @TheRaceDraft: Karate bitch https://t.co/Ut...   \n",
      "2  When ratchet bitches find out the club ain't 1...   \n",
      "3  RT @trevso_electric: You want a hot body? You ...   \n",
      "4                    We don't pop out bitch we slide   \n",
      "\n",
      "                                         tweet_clean  \n",
      "0                                      rt bird throw  \n",
      "1                                    rt karate bitch  \n",
      "2                   ratchet bitches find club ai get  \n",
      "3  rt want hot body want bugatti better born weal...  \n",
      "4                                    pop bitch slide  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- file input ---\n",
    "TEST_FILE = r\"C:\\Users\\PC\\CS221\\Inference_3_pipeline\\test_100_stratified_from_test.csv\"   \n",
    "SHEET_NAME = None                # ví dụ: \"Sheet1\" nếu cần\n",
    "\n",
    "if TEST_FILE.lower().endswith(\".xlsx\"):\n",
    "    tmp = pd.read_excel(TEST_FILE, sheet_name=SHEET_NAME)  # SHEET_NAME có thể None\n",
    "    # Nếu sheet_name=None => tmp là dict {sheet: df}\n",
    "    if isinstance(tmp, dict):\n",
    "        # lấy sheet đầu tiên\n",
    "        first_sheet = list(tmp.keys())[0]\n",
    "        print(\"Detected multiple sheets. Using sheet:\", first_sheet)\n",
    "        df = tmp[first_sheet]\n",
    "    else:\n",
    "        df = tmp\n",
    "else:\n",
    "    df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(\"df type:\", type(df))\n",
    "print(\"columns:\", list(df.columns))\n",
    "\n",
    "# --- columns ---\n",
    "TEXT_COL = \"Tweet\"\n",
    "LABEL_COL = \"Final Votes\"  # nếu không có label: set = None\n",
    "\n",
    "assert TEXT_COL in df.columns, f\"Thiếu cột '{TEXT_COL}' trong file test!\"\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "for pkg in [\"punkt\", \"stopwords\"]:\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)                   # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)                   # Remove hashtags\n",
    "    text = text.lower()                                # Lowercase\n",
    "    tokens = word_tokenize(text)                       # Tokenize\n",
    "    filtered_tokens = [\n",
    "        w for w in tokens\n",
    "        if w.isalpha() and w not in STOPWORDS          # Remove stopwords + non-alpha\n",
    "    ]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# --- apply cleaning ---\n",
    "df[\"tweet_clean\"] = df[TEXT_COL].apply(clean_text)\n",
    "\n",
    "print(\"Data after cleaning:\")\n",
    "print(df[[TEXT_COL, \"tweet_clean\"]].head())\n",
    "\n",
    "# list texts to feed models\n",
    "texts = df[\"tweet_clean\"].astype(str).fillna(\"\").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ca9b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Paths tới 3 pipeline artifacts ----------\n",
    "# ML artifacts (từ notebook: tfidf.pkl + *_tuned.pkl)\n",
    "ML_DIR   = \"random_forest_models\"  \n",
    "PIPE_ML_PKL = r\"C:\\Users\\PC\\CS221\\outputs_ML\\pipe_rf.pkl\"\n",
    "\n",
    "# RNN artifacts (từ notebook: tokenizer_rnn.pkl + *.keras)\n",
    "RNN_DIR = \"rnn_lstm_models\"  \n",
    "TOKENIZER_PKL   = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\tokenizer_lstm1d.pkl\"\n",
    "RNN_MODEL_KERAS = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\lstm1d_hate_speech.keras\"\n",
    "\n",
    "# BERT artifacts (từ notebook: checkpoints/bert_best)\n",
    "BERT_DIR = r\"C:\\Users\\PC\\CS221\\outputs_bert\\bert_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e11e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(x: str) -> str:\n",
    "    x = str(x).strip().lower()\n",
    "    if x in [\"hate\", \"hate_speech\", \"hate speech\", \"hatespeech\"]:\n",
    "        return \"Hate\"\n",
    "    if x in [\"offensive\", \"offensive_language\", \"offensive language\", \"offensivelanguage\"]:\n",
    "        return \"Offensive\"\n",
    "    if x in [\"neither\", \"neutral\", \"none\", \"clean\"]:\n",
    "        return \"Neither\"\n",
    "    return str(x)\n",
    "\n",
    "ID2LABEL_ML = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c50d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ml(texts, pipe_path=PIPE_ML_PKL):\n",
    "    \"\"\"\n",
    "    Load 1 sklearn Pipeline (.pkl) đã chứa tfidf + classifier.\n",
    "    Return:\n",
    "      - preds: list[str] (Hate/Offensive/Neither hoặc label gốc của model)\n",
    "      - proba: np.ndarray shape (n,3) hoặc None nếu model không hỗ trợ\n",
    "    \"\"\"\n",
    "    pipe = joblib.load(pipe_path)\n",
    "\n",
    "    # sklearn pipeline có thể predict ra string labels luôn (vì bạn train y là \"Hate\"/\"Offensive\"/\"neither\")\n",
    "    y_pred = pipe.predict(texts)\n",
    "\n",
    "    # normalize về chuẩn bạn dùng trong report\n",
    "    preds = [normalize_label(x) for x in y_pred]\n",
    "\n",
    "    proba = None\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(texts)\n",
    "    return preds, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ece6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def predict_bert(texts, batch_size=32, max_len=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_DIR)\n",
    "    model = BertForSequenceClassification.from_pretrained(BERT_DIR)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_probs = [], []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            pred_ids = probs.argmax(axis=1)\n",
    "\n",
    "        # theo notebook BERT: label2id = {'hate_speech':0,'offensive_language':1,'neither':2}\n",
    "        ID2LABEL_BERT = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}\n",
    "        all_preds.extend([ID2LABEL_BERT[int(j)] for j in pred_ids])\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    return all_preds, np.vstack(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37a02b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df) = 100\n",
      "len(pred_ml)  = 100\n",
      "len(pred_rnn) = 99\n",
      "len(pred_bert)= 100\n"
     ]
    }
   ],
   "source": [
    "print(\"len(df) =\", len(df))\n",
    "print(\"len(pred_ml)  =\", len(pred_ml))\n",
    "print(\"len(pred_rnn) =\", len(pred_rnn))\n",
    "print(\"len(pred_bert)=\", len(pred_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff462838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\PC\\CS221\\preds_3pipelines_dataset.csv\n",
      "\n",
      "=== ML ===\n",
      "Accuracy: 0.72\n",
      "Macro-F1: 0.5725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.2857    0.3333    0.3077         6\n",
      "   Offensive     0.9474    0.7013    0.8060        77\n",
      "     Neither     0.4444    0.9412    0.6038        17\n",
      "\n",
      "    accuracy                         0.7200       100\n",
      "   macro avg     0.5592    0.6586    0.5725       100\n",
      "weighted avg     0.8222    0.7200    0.7417       100\n",
      "\n",
      "\n",
      "=== RNN ===\n",
      "Accuracy: 0.5\n",
      "Macro-F1: 0.3075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.0000    0.0000    0.0000         6\n",
      "   Offensive     0.7857    0.5714    0.6617        77\n",
      "     Neither     0.2069    0.3529    0.2609        17\n",
      "\n",
      "    accuracy                         0.5000       100\n",
      "   macro avg     0.3309    0.3081    0.3075       100\n",
      "weighted avg     0.6402    0.5000    0.5538       100\n",
      "\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.95\n",
      "Macro-F1: 0.8377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.6000    0.5000    0.5455         6\n",
      "   Offensive     0.9615    0.9740    0.9677        77\n",
      "     Neither     1.0000    1.0000    1.0000        17\n",
      "\n",
      "    accuracy                         0.9500       100\n",
      "   macro avg     0.8538    0.8247    0.8377       100\n",
      "weighted avg     0.9464    0.9500    0.9479       100\n",
      "\n",
      "Saved: C:\\Users\\PC\\CS221\\disagree_cases_dataset.csv | n_disagree = 66\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RNN_PREDS_CSV = r\"C:\\Users\\PC\\CS221\\rnn_preds.csv\"   # file bạn đã export\n",
    "\n",
    "# 1) ML + BERT inference\n",
    "pred_ml, proba_ml     = predict_ml(texts)\n",
    "pred_bert, proba_bert = predict_bert(texts)\n",
    "\n",
    "# 2) Load RNN preds từ CSV rồi merge theo thứ tự dòng\n",
    "rnn_df = pd.read_csv(RNN_PREDS_CSV)\n",
    "\n",
    "# cố gắng đoán tên cột dự đoán trong file rnn_preds.csv\n",
    "cand_cols = [c for c in [\"pred_RNN\", \"rnn_pred\", \"pred\", \"prediction\", \"label\"] if c in rnn_df.columns]\n",
    "if len(cand_cols) == 0:\n",
    "    # fallback: lấy cột đầu tiên\n",
    "    rnn_col = rnn_df.columns[0]\n",
    "else:\n",
    "    rnn_col = cand_cols[0]\n",
    "\n",
    "pred_rnn = rnn_df[rnn_col].astype(str).tolist()\n",
    "\n",
    "# check số dòng khớp\n",
    "assert len(pred_rnn) == len(df), f\"RNN preds rows ({len(pred_rnn)}) != df rows ({len(df)})\"\n",
    "\n",
    "# 3) Build output table\n",
    "out = df.copy()\n",
    "out[\"pred_ML\"]   = pred_ml\n",
    "out[\"pred_RNN\"]  = pred_rnn\n",
    "out[\"pred_BERT\"] = pred_bert\n",
    "\n",
    "# normalize label thật nếu có\n",
    "if LABEL_COL and (LABEL_COL in out.columns):\n",
    "    out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 4) Save merged predictions\n",
    "out_path = r\"C:\\Users\\PC\\CS221\\preds_3pipelines_dataset.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# 5) Evaluation (nếu có y_true)\n",
    "if \"y_true\" in out.columns:\n",
    "    y_true = out[\"y_true\"].tolist()\n",
    "    labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "    def eval_one(name, y_pred):\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "        print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "        print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "    eval_one(\"ML\",   out[\"pred_ML\"].tolist())\n",
    "    eval_one(\"RNN\",  out[\"pred_RNN\"].tolist())\n",
    "    eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "\n",
    "# 6) Disagreement analysis (không cần y_true)\n",
    "out[\"all_agree\"] = (out[\"pred_ML\"] == out[\"pred_RNN\"]) & (out[\"pred_RNN\"] == out[\"pred_BERT\"])\n",
    "disagree = out[~out[\"all_agree\"]].copy()\n",
    "\n",
    "disagree_path = r\"C:\\Users\\PC\\CS221\\disagree_cases_dataset.csv\"\n",
    "disagree.to_csv(disagree_path, index=False)\n",
    "print(\"Saved:\", disagree_path, \"| n_disagree =\", len(disagree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b49b7768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Tweet', 'Final Votes', 'tweet_clean', 'pred_ML', 'pred_RNN', 'pred_BERT', 'y_true', 'all_agree']\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.95\n",
      "Macro-F1: 0.8377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.6000    0.5000    0.5455         6\n",
      "   Offensive     0.9615    0.9740    0.9677        77\n",
      "     Neither     1.0000    1.0000    1.0000        17\n",
      "\n",
      "    accuracy                         0.9500       100\n",
      "   macro avg     0.8538    0.8247    0.8377       100\n",
      "weighted avg     0.9464    0.9500    0.9479       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# 1) check cột label thật\n",
    "print(\"Columns:\", list(out.columns))\n",
    "assert LABEL_COL in out.columns, f\"File test không có cột label thật '{LABEL_COL}'\"\n",
    "assert \"pred_BERT\" in out.columns, \"Chưa có cột pred_BERT (chưa chạy predict_bert hoặc chưa merge)\"\n",
    "\n",
    "# 2) tạo y_true chuẩn\n",
    "out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 3) eval riêng cho BERT (và in luôn ML/RNN nếu muốn)\n",
    "labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "y_true = out[\"y_true\"].tolist()\n",
    "\n",
    "def eval_one(name, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "    print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "    print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "# eval_one(\"ML\", out[\"pred_ML\"].tolist())\n",
    "# eval_one(\"RNN\", out[\"pred_RNN\"].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate-bert-ig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
