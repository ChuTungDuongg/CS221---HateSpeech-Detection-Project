{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33da5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\pc\\miniconda3\\envs\\hate-bert-ig\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\pc\\miniconda3\\envs\\hate-bert-ig\\lib\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "import os, re, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import openpyxl\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "\n",
    "needed = [\n",
    "    (\"tokenizers/punkt\", \"punkt\"),\n",
    "    (\"tokenizers/punkt_tab\", \"punkt_tab\"),   # <-- thêm cái này\n",
    "    (\"corpora/stopwords\", \"stopwords\"),\n",
    "]\n",
    "\n",
    "for path, pkg in needed:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf629d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected multiple sheets. Using sheet: Trang tính1\n",
      "df type: <class 'pandas.core.frame.DataFrame'>\n",
      "columns: ['STT', 'Tweet', 'Final Votes']\n",
      "Data after cleaning:\n",
      "                                               Tweet  \\\n",
      "0   @IamReagzo Ohh now my stupid brain was somewhere   \n",
      "1          @vesperdigital You're obviously an idiot.   \n",
      "2  @Hankers @tleehumphrey @primetimecrime @cbcwat...   \n",
      "3  @lekside34 Since your father is not dumb, let ...   \n",
      "4                   @RaymondRowan7 Who gives a fuck.   \n",
      "\n",
      "                                         tweet_clean  \n",
      "0                         ohh stupid brain somewhere  \n",
      "1                                    obviously idiot  \n",
      "2                                         fuck sakes  \n",
      "3  since father dumb let speak like wole soyinka ...  \n",
      "4                                         gives fuck  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- file input ---\n",
    "TEST_FILE = \"100_samples.xlsx\"   # hoặc \"custom_100.csv\"\n",
    "SHEET_NAME = None                # ví dụ: \"Sheet1\" nếu cần\n",
    "\n",
    "if TEST_FILE.lower().endswith(\".xlsx\"):\n",
    "    tmp = pd.read_excel(TEST_FILE, sheet_name=SHEET_NAME)  # SHEET_NAME có thể None\n",
    "    # Nếu sheet_name=None => tmp là dict {sheet: df}\n",
    "    if isinstance(tmp, dict):\n",
    "        # lấy sheet đầu tiên\n",
    "        first_sheet = list(tmp.keys())[0]\n",
    "        print(\"Detected multiple sheets. Using sheet:\", first_sheet)\n",
    "        df = tmp[first_sheet]\n",
    "    else:\n",
    "        df = tmp\n",
    "else:\n",
    "    df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(\"df type:\", type(df))\n",
    "print(\"columns:\", list(df.columns))\n",
    "\n",
    "# --- columns ---\n",
    "TEXT_COL = \"Tweet\"\n",
    "LABEL_COL = \"Final Votes\"  # nếu không có label: set = None\n",
    "\n",
    "assert TEXT_COL in df.columns, f\"Thiếu cột '{TEXT_COL}' trong file test!\"\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "for pkg in [\"punkt\", \"stopwords\"]:\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)                   # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)                   # Remove hashtags\n",
    "    text = text.lower()                                # Lowercase\n",
    "    tokens = word_tokenize(text)                       # Tokenize\n",
    "    filtered_tokens = [\n",
    "        w for w in tokens\n",
    "        if w.isalpha() and w not in STOPWORDS          # Remove stopwords + non-alpha\n",
    "    ]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# --- apply cleaning ---\n",
    "df[\"tweet_clean\"] = df[TEXT_COL].apply(clean_text)\n",
    "\n",
    "print(\"Data after cleaning:\")\n",
    "print(df[[TEXT_COL, \"tweet_clean\"]].head())\n",
    "\n",
    "# list texts to feed models\n",
    "texts = df[\"tweet_clean\"].astype(str).fillna(\"\").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ca9b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Paths tới 3 pipeline artifacts ----------\n",
    "# ML artifacts (từ notebook: tfidf.pkl + *_tuned.pkl)\n",
    "ML_DIR   = \"random_forest_models\"  \n",
    "PIPE_ML_PKL = r\"C:\\Users\\PC\\CS221\\outputs_ML\\pipe_rf.pkl\"\n",
    "\n",
    "# RNN artifacts (từ notebook: tokenizer_rnn.pkl + *.keras)\n",
    "RNN_DIR = \"rnn_lstm_models\"  \n",
    "TOKENIZER_PKL   = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\tokenizer_lstm1d.pkl\"\n",
    "RNN_MODEL_KERAS = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\lstm1d_hate_speech.keras\"\n",
    "\n",
    "# BERT artifacts (từ notebook: checkpoints/bert_best)\n",
    "BERT_DIR = r\"C:\\Users\\PC\\CS221\\outputs_bert\\bert_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e11e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(x: str) -> str:\n",
    "    x = str(x).strip().lower()\n",
    "    if x in [\"hate\", \"hate_speech\", \"hate speech\", \"hatespeech\"]:\n",
    "        return \"Hate\"\n",
    "    if x in [\"offensive\", \"offensive_language\", \"offensive language\", \"offensivelanguage\"]:\n",
    "        return \"Offensive\"\n",
    "    if x in [\"neither\", \"neutral\", \"none\", \"clean\"]:\n",
    "        return \"Neither\"\n",
    "    return str(x)\n",
    "\n",
    "ID2LABEL_ML = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c50d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ml(texts, pipe_path=PIPE_ML_PKL):\n",
    "    \"\"\"\n",
    "    Load 1 sklearn Pipeline (.pkl) đã chứa tfidf + classifier.\n",
    "    Return:\n",
    "      - preds: list[str] (Hate/Offensive/Neither hoặc label gốc của model)\n",
    "      - proba: np.ndarray shape (n,3) hoặc None nếu model không hỗ trợ\n",
    "    \"\"\"\n",
    "    pipe = joblib.load(pipe_path)\n",
    "\n",
    "    # sklearn pipeline có thể predict ra string labels luôn (vì bạn train y là \"Hate\"/\"Offensive\"/\"neither\")\n",
    "    y_pred = pipe.predict(texts)\n",
    "\n",
    "    # normalize về chuẩn bạn dùng trong report\n",
    "    preds = [normalize_label(x) for x in y_pred]\n",
    "\n",
    "    proba = None\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(texts)\n",
    "    return preds, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ece6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def predict_bert(texts, batch_size=32, max_len=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_DIR)\n",
    "    model = BertForSequenceClassification.from_pretrained(BERT_DIR)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_probs = [], []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            pred_ids = probs.argmax(axis=1)\n",
    "\n",
    "        # theo notebook BERT: label2id = {'hate_speech':0,'offensive_language':1,'neither':2}\n",
    "        ID2LABEL_BERT = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}\n",
    "        all_preds.extend([ID2LABEL_BERT[int(j)] for j in pred_ids])\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    return all_preds, np.vstack(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff462838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\PC\\CS221\\preds_3pipelines.csv\n",
      "\n",
      "=== ML ===\n",
      "Accuracy: 0.7071\n",
      "Macro-F1: 0.3893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.2000    0.1667    0.1818         6\n",
      "   Offensive     1.0000    0.0833    0.1538        24\n",
      "     Neither     0.7283    0.9710    0.8323        69\n",
      "\n",
      "    accuracy                         0.7071        99\n",
      "   macro avg     0.6428    0.4070    0.3893        99\n",
      "weighted avg     0.7621    0.7071    0.6284        99\n",
      "\n",
      "\n",
      "=== RNN ===\n",
      "Accuracy: 0.6566\n",
      "Macro-F1: 0.288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.0000    0.0000    0.0000         6\n",
      "   Offensive     0.2000    0.0417    0.0690        24\n",
      "     Neither     0.6957    0.9275    0.7950        69\n",
      "\n",
      "    accuracy                         0.6566        99\n",
      "   macro avg     0.2986    0.3231    0.2880        99\n",
      "weighted avg     0.5333    0.6566    0.5708        99\n",
      "\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.6667\n",
      "Macro-F1: 0.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.3333    0.5000    0.4000         6\n",
      "   Offensive     0.2500    0.1250    0.1667        24\n",
      "     Neither     0.7692    0.8696    0.8163        69\n",
      "\n",
      "    accuracy                         0.6667        99\n",
      "   macro avg     0.4509    0.4982    0.4610        99\n",
      "weighted avg     0.6169    0.6667    0.6336        99\n",
      "\n",
      "Saved: C:\\Users\\PC\\CS221\\disagree_cases.csv | n_disagree = 31\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RNN_PREDS_CSV = r\"C:\\Users\\PC\\CS221\\rnn_preds.csv\"   # file bạn đã export\n",
    "\n",
    "# 1) ML + BERT inference\n",
    "pred_ml, proba_ml     = predict_ml(texts)\n",
    "pred_bert, proba_bert = predict_bert(texts)\n",
    "\n",
    "# 2) Load RNN preds từ CSV rồi merge theo thứ tự dòng\n",
    "rnn_df = pd.read_csv(RNN_PREDS_CSV)\n",
    "\n",
    "# cố gắng đoán tên cột dự đoán trong file rnn_preds.csv\n",
    "cand_cols = [c for c in [\"pred_RNN\", \"rnn_pred\", \"pred\", \"prediction\", \"label\"] if c in rnn_df.columns]\n",
    "if len(cand_cols) == 0:\n",
    "    # fallback: lấy cột đầu tiên\n",
    "    rnn_col = rnn_df.columns[0]\n",
    "else:\n",
    "    rnn_col = cand_cols[0]\n",
    "\n",
    "pred_rnn = rnn_df[rnn_col].astype(str).tolist()\n",
    "\n",
    "# check số dòng khớp\n",
    "assert len(pred_rnn) == len(df), f\"RNN preds rows ({len(pred_rnn)}) != df rows ({len(df)})\"\n",
    "\n",
    "# 3) Build output table\n",
    "out = df.copy()\n",
    "out[\"pred_ML\"]   = pred_ml\n",
    "out[\"pred_RNN\"]  = pred_rnn\n",
    "out[\"pred_BERT\"] = pred_bert\n",
    "\n",
    "# normalize label thật nếu có\n",
    "if LABEL_COL and (LABEL_COL in out.columns):\n",
    "    out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 4) Save merged predictions\n",
    "out_path = r\"C:\\Users\\PC\\CS221\\preds_3pipelines.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# 5) Evaluation (nếu có y_true)\n",
    "if \"y_true\" in out.columns:\n",
    "    y_true = out[\"y_true\"].tolist()\n",
    "    labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "    def eval_one(name, y_pred):\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "        print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "        print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "    eval_one(\"ML\",   out[\"pred_ML\"].tolist())\n",
    "    eval_one(\"RNN\",  out[\"pred_RNN\"].tolist())\n",
    "    eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "\n",
    "# 6) Disagreement analysis (không cần y_true)\n",
    "out[\"all_agree\"] = (out[\"pred_ML\"] == out[\"pred_RNN\"]) & (out[\"pred_RNN\"] == out[\"pred_BERT\"])\n",
    "disagree = out[~out[\"all_agree\"]].copy()\n",
    "\n",
    "disagree_path = r\"C:\\Users\\PC\\CS221\\disagree_cases.csv\"\n",
    "disagree.to_csv(disagree_path, index=False)\n",
    "print(\"Saved:\", disagree_path, \"| n_disagree =\", len(disagree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b49b7768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['STT', 'Tweet', 'Final Votes', 'tweet_clean', 'pred_ML', 'pred_RNN', 'pred_BERT', 'y_true', 'all_agree']\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.6667\n",
      "Macro-F1: 0.461\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.3333    0.5000    0.4000         6\n",
      "   Offensive     0.2500    0.1250    0.1667        24\n",
      "     Neither     0.7692    0.8696    0.8163        69\n",
      "\n",
      "    accuracy                         0.6667        99\n",
      "   macro avg     0.4509    0.4982    0.4610        99\n",
      "weighted avg     0.6169    0.6667    0.6336        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# 1) check cột label thật\n",
    "print(\"Columns:\", list(out.columns))\n",
    "assert LABEL_COL in out.columns, f\"File test không có cột label thật '{LABEL_COL}'\"\n",
    "assert \"pred_BERT\" in out.columns, \"Chưa có cột pred_BERT (chưa chạy predict_bert hoặc chưa merge)\"\n",
    "\n",
    "# 2) tạo y_true chuẩn\n",
    "out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 3) eval riêng cho BERT (và in luôn ML/RNN nếu muốn)\n",
    "labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "y_true = out[\"y_true\"].tolist()\n",
    "\n",
    "def eval_one(name, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "    print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "    print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "# eval_one(\"ML\", out[\"pred_ML\"].tolist())\n",
    "# eval_one(\"RNN\", out[\"pred_RNN\"].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate-bert-ig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
