{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33da5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\miniconda3\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\miniconda3\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\miniconda3\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\miniconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk \n",
    "import os, re, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import openpyxl\n",
    "import html\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "\n",
    "needed = [\n",
    "    (\"tokenizers/punkt\", \"punkt\"),\n",
    "    (\"tokenizers/punkt_tab\", \"punkt_tab\"),   # <-- thêm cái này\n",
    "    (\"corpora/stopwords\", \"stopwords\"),\n",
    "]\n",
    "\n",
    "for path, pkg in needed:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf629d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df type: <class 'pandas.core.frame.DataFrame'>\n",
      "columns: ['Tweet', 'Final Votes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after cleaning:\n",
      "                                               Tweet  \\\n",
      "0  RT @BirdGang316: If you a bird throw it up #bi...   \n",
      "1  RT @TheRaceDraft: Karate bitch https://t.co/Ut...   \n",
      "2  When ratchet bitches find out the club ain't 1...   \n",
      "3  RT @trevso_electric: You want a hot body? You ...   \n",
      "4                    We don't pop out bitch we slide   \n",
      "\n",
      "                                         tweet_clean  \n",
      "0                                      rt bird throw  \n",
      "1                                    rt karate bitch  \n",
      "2                   ratchet bitches find club ai get  \n",
      "3  rt want hot body want bugatti better born weal...  \n",
      "4                                    pop bitch slide  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- file input ---\n",
    "TEST_FILE = r\"C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\Inference_3_pipeline\\test_100_stratified_from_test.csv\"   \n",
    "SHEET_NAME = None                # ví dụ: \"Sheet1\" nếu cần\n",
    "\n",
    "if TEST_FILE.lower().endswith(\".xlsx\"):\n",
    "    tmp = pd.read_excel(TEST_FILE, sheet_name=SHEET_NAME)  # SHEET_NAME có thể None\n",
    "    # Nếu sheet_name=None => tmp là dict {sheet: df}\n",
    "    if isinstance(tmp, dict):\n",
    "        # lấy sheet đầu tiên\n",
    "        first_sheet = list(tmp.keys())[0]\n",
    "        print(\"Detected multiple sheets. Using sheet:\", first_sheet)\n",
    "        df = tmp[first_sheet]\n",
    "    else:\n",
    "        df = tmp\n",
    "else:\n",
    "    df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(\"df type:\", type(df))\n",
    "print(\"columns:\", list(df.columns))\n",
    "\n",
    "# --- columns ---\n",
    "TEXT_COL = \"Tweet\"\n",
    "LABEL_COL = \"Final Votes\"  # nếu không có label: set = None\n",
    "\n",
    "assert TEXT_COL in df.columns, f\"Thiếu cột '{TEXT_COL}' trong file test!\"\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "for pkg in [\"punkt\", \"stopwords\"]:\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)                   # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)                   # Remove hashtags\n",
    "    text = text.lower()                                # Lowercase\n",
    "    tokens = word_tokenize(text)                       # Tokenize\n",
    "    filtered_tokens = [\n",
    "        w for w in tokens\n",
    "        if w.isalpha() and w not in STOPWORDS          # Remove stopwords + non-alpha\n",
    "    ]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "USER_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#(\\w+)')  # giữ chữ trong hashtag\n",
    "\n",
    "def clean_text_bert(text):\n",
    "    text = str(text)\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Thay vì xóa sạch, dùng token để giữ tín hiệu \"có URL/mention\"\n",
    "    text = URL_RE.sub(' [URL] ', text)\n",
    "    text = USER_RE.sub(' [USER] ', text)\n",
    "\n",
    "    # Giữ nội dung hashtag, chỉ bỏ dấu #\n",
    "    text = HASHTAG_RE.sub(r' \\1 ', text)\n",
    "\n",
    "    # Chuẩn hóa khoảng trắng\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Chỉ bật nếu bạn dùng model uncased (vd: bert-base-uncased)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# --- apply cleaning ---\n",
    "df[\"tweet_clean\"] = df[TEXT_COL].apply(clean_text)\n",
    "\n",
    "print(\"Data after cleaning:\")\n",
    "print(df[[TEXT_COL, \"tweet_clean\"]].head())\n",
    "\n",
    "# list texts to feed models\n",
    "texts = df[TEXT_COL].apply(clean_text).astype(str).fillna(\"\").tolist()\n",
    "texts_bert = df[TEXT_COL].apply(clean_text_bert).astype(str).fillna(\"\").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca9b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Paths tới 3 pipeline artifacts ----------\n",
    "# ML artifacts (từ notebook: tfidf.pkl + *_tuned.pkl)\n",
    "ML_DIR   = \"random_forest_models\"  \n",
    "PIPE_ML_PKL = r\"C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\outputs_ML\\pipe_rf.pkl\"\n",
    "\n",
    "# RNN artifacts (từ notebook: tokenizer_rnn.pkl + *.keras)\n",
    "RNN_DIR = \"rnn_lstm_models\"  \n",
    "TOKENIZER_PKL   = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\tokenizer_lstm1d.pkl\"\n",
    "RNN_MODEL_KERAS = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\lstm1d_hate_speech.keras\"\n",
    "\n",
    "# BERT artifacts (từ notebook: checkpoints/bert_best)\n",
    "BERT_DIR = r\"C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\outputs_bert\\bert_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e11e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(x: str) -> str:\n",
    "    x = str(x).strip().lower()\n",
    "    if x in [\"hate\", \"hate_speech\", \"hate speech\", \"hatespeech\"]:\n",
    "        return \"Hate Speech\"\n",
    "    if x in [\"offensive\", \"offensive_language\", \"offensive language\", \"offensivelanguage\"]:\n",
    "        return \"Offensive\"\n",
    "    if x in [\"neither\", \"neutral\", \"none\", \"clean\"]:\n",
    "        return \"Neither\"\n",
    "    return str(x)\n",
    "\n",
    "ID2LABEL_ML = {0: \"Hate Speech\", 1: \"Offensive\", 2: \"Neither\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c50d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ml(texts, pipe_path=PIPE_ML_PKL):\n",
    "    \"\"\"\n",
    "    Load 1 sklearn Pipeline (.pkl) đã chứa tfidf + classifier.\n",
    "    Return:\n",
    "      - preds: list[str] (Hate/Offensive/Neither hoặc label gốc của model)\n",
    "      - proba: np.ndarray shape (n,3) hoặc None nếu model không hỗ trợ\n",
    "    \"\"\"\n",
    "    pipe = joblib.load(pipe_path)\n",
    "\n",
    "    # sklearn pipeline có thể predict ra string labels luôn (vì bạn train y là \"Hate\"/\"Offensive\"/\"neither\")\n",
    "    y_pred = pipe.predict(texts)\n",
    "\n",
    "    # normalize về chuẩn bạn dùng trong report\n",
    "    preds = [normalize_label(x) for x in y_pred]\n",
    "\n",
    "    proba = None\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(texts)\n",
    "    return preds, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ece6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def predict_bert(texts, batch_size=32, max_len=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_DIR)\n",
    "    model = BertForSequenceClassification.from_pretrained(BERT_DIR)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_probs = [], []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            pred_ids = probs.argmax(axis=1)\n",
    "\n",
    "        # theo notebook BERT: label2id = {'hate_speech':0,'offensive_language':1,'neither':2}\n",
    "        ID2LABEL_BERT = {0: \"Hate Speech\", 1: \"Offensive\", 2: \"Neither\"}\n",
    "        all_preds.extend([ID2LABEL_BERT[int(j)] for j in pred_ids])\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    return all_preds, np.vstack(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37a02b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df) = 100\n",
      "len(pred_ml)  = 100\n",
      "len(pred_rnn) = 100\n",
      "len(pred_bert)= 100\n"
     ]
    }
   ],
   "source": [
    "print(\"len(df) =\", len(df))\n",
    "print(\"len(pred_ml)  =\", len(pred_ml))\n",
    "print(\"len(pred_rnn) =\", len(pred_rnn))\n",
    "print(\"len(pred_bert)=\", len(pred_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff462838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\preds_3pipelines_dataset.csv\n",
      "\n",
      "=== ML ===\n",
      "Accuracy: 0.74\n",
      "Macro-F1: 0.6054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Hate Speech     0.5000    0.3333    0.4000         6\n",
      "   Offensive     0.9492    0.7273    0.8235        77\n",
      "     Neither     0.4324    0.9412    0.5926        17\n",
      "\n",
      "    accuracy                         0.7400       100\n",
      "   macro avg     0.6272    0.6673    0.6054       100\n",
      "weighted avg     0.8344    0.7400    0.7589       100\n",
      "\n",
      "\n",
      "=== RNN ===\n",
      "Accuracy: 0.89\n",
      "Macro-F1: 0.7784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Hate Speech     0.3636    0.6667    0.4706         6\n",
      "   Offensive     0.9459    0.9091    0.9272        77\n",
      "     Neither     1.0000    0.8824    0.9375        17\n",
      "\n",
      "    accuracy                         0.8900       100\n",
      "   macro avg     0.7699    0.8194    0.7784       100\n",
      "weighted avg     0.9202    0.8900    0.9015       100\n",
      "\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.95\n",
      "Macro-F1: 0.8464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Hate Speech     0.7500    0.5000    0.6000         6\n",
      "   Offensive     0.9615    0.9740    0.9677        77\n",
      "     Neither     0.9444    1.0000    0.9714        17\n",
      "\n",
      "    accuracy                         0.9500       100\n",
      "   macro avg     0.8853    0.8247    0.8464       100\n",
      "weighted avg     0.9459    0.9500    0.9463       100\n",
      "\n",
      "Saved: C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\disagree_cases_dataset.csv | n_disagree = 31\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RNN_PREDS_CSV = r\"C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\rnn_preds.csv\"   # file bạn đã export\n",
    "\n",
    "# 1) ML + BERT inference\n",
    "pred_ml, proba_ml     = predict_ml(texts)\n",
    "pred_bert, proba_bert = predict_bert(texts_bert)\n",
    "\n",
    "# 2) Load RNN preds từ CSV rồi merge theo thứ tự dòng\n",
    "rnn_df = pd.read_csv(RNN_PREDS_CSV)\n",
    "\n",
    "# cố gắng đoán tên cột dự đoán trong file rnn_preds.csv\n",
    "cand_cols = [c for c in [\"pred_RNN\", \"rnn_pred\", \"pred\", \"prediction\", \"label\"] if c in rnn_df.columns]\n",
    "if len(cand_cols) == 0:\n",
    "    # fallback: lấy cột đầu tiên\n",
    "    rnn_col = rnn_df.columns[0]\n",
    "else:\n",
    "    rnn_col = cand_cols[0]\n",
    "\n",
    "pred_rnn = rnn_df[rnn_col].astype(str).tolist()\n",
    "\n",
    "# check số dòng khớp\n",
    "assert len(pred_rnn) == len(df), f\"RNN preds rows ({len(pred_rnn)}) != df rows ({len(df)})\"\n",
    "\n",
    "# 3) Build output table\n",
    "out = df.copy()\n",
    "out[\"pred_ML\"]   = pred_ml\n",
    "out[\"pred_RNN\"]  = pred_rnn\n",
    "out[\"pred_BERT\"] = pred_bert\n",
    "\n",
    "# normalize label thật nếu có\n",
    "if LABEL_COL and (LABEL_COL in out.columns):\n",
    "    out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 4) Save merged predictions\n",
    "out_path = r\"C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\preds_3pipelines_dataset.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# 5) Evaluation (nếu có y_true)\n",
    "if \"y_true\" in out.columns:\n",
    "    y_true = out[\"y_true\"].tolist()\n",
    "    labels = [\"Hate Speech\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "    def eval_one(name, y_pred):\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "        print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "        print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "    eval_one(\"ML\",   out[\"pred_ML\"].tolist())\n",
    "    eval_one(\"RNN\",  out[\"pred_RNN\"].tolist())\n",
    "    eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "\n",
    "# 6) Disagreement analysis (không cần y_true)\n",
    "out[\"all_agree\"] = (out[\"pred_ML\"] == out[\"pred_RNN\"]) & (out[\"pred_RNN\"] == out[\"pred_BERT\"])\n",
    "disagree = out[~out[\"all_agree\"]].copy()\n",
    "\n",
    "disagree_path = r\"C:\\Users\\ADMIN\\CS221---HateSpeech-Detection-Project\\disagree_cases_dataset.csv\"\n",
    "disagree.to_csv(disagree_path, index=False)\n",
    "print(\"Saved:\", disagree_path, \"| n_disagree =\", len(disagree))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
