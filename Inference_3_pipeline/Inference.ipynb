{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33da5f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "import os, re, pickle, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf629d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m TEXT_COL = \u001b[33m\"\u001b[39m\u001b[33mTweet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m LABEL_COL = \u001b[33m\"\u001b[39m\u001b[33mFinal Votes\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# nếu không có label: set = None\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m TEXT_COL \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThiếu cột \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEXT_COL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m trong file test!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# --- nltk resources (local) ---\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pkg \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- file input ---\n",
    "TEST_FILE = \"100_samples.xlsx\"   # hoặc \"custom_100.csv\"\n",
    "SHEET_NAME = None                # ví dụ: \"Sheet1\" nếu cần\n",
    "\n",
    "if TEST_FILE.lower().endswith(\".xlsx\"):\n",
    "    tmp = pd.read_excel(TEST_FILE, sheet_name=SHEET_NAME)  # SHEET_NAME có thể None\n",
    "    # Nếu sheet_name=None => tmp là dict {sheet: df}\n",
    "    if isinstance(tmp, dict):\n",
    "        # lấy sheet đầu tiên\n",
    "        first_sheet = list(tmp.keys())[0]\n",
    "        print(\"Detected multiple sheets. Using sheet:\", first_sheet)\n",
    "        df = tmp[first_sheet]\n",
    "    else:\n",
    "        df = tmp\n",
    "else:\n",
    "    df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "print(\"df type:\", type(df))\n",
    "print(\"columns:\", list(df.columns))\n",
    "\n",
    "# --- columns ---\n",
    "TEXT_COL = \"Tweet\"\n",
    "LABEL_COL = \"Final Votes\"  # nếu không có label: set = None\n",
    "\n",
    "assert TEXT_COL in df.columns, f\"Thiếu cột '{TEXT_COL}' trong file test!\"\n",
    "\n",
    "# --- nltk resources (local) ---\n",
    "for pkg in [\"punkt\", \"stopwords\"]:\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)                   # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)                   # Remove hashtags\n",
    "    text = text.lower()                                # Lowercase\n",
    "    tokens = word_tokenize(text)                       # Tokenize\n",
    "    filtered_tokens = [\n",
    "        w for w in tokens\n",
    "        if w.isalpha() and w not in STOPWORDS          # Remove stopwords + non-alpha\n",
    "    ]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# --- apply cleaning ---\n",
    "df[\"tweet_clean\"] = df[TEXT_COL].apply(clean_text)\n",
    "\n",
    "print(\"Data after cleaning:\")\n",
    "print(df[[TEXT_COL, \"tweet_clean\"]].head())\n",
    "\n",
    "# list texts to feed models\n",
    "texts = df[\"tweet_clean\"].astype(str).fillna(\"\").tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1) Paths tới 3 pipeline artifacts ----------\n",
    "# ML artifacts (từ notebook: tfidf.pkl + *_tuned.pkl)\n",
    "ML_DIR   = \"random_forest_models\"  \n",
    "PIPE_ML_PKL = r\"C:\\Users\\PC\\CS221\\outputs_ML\\pipe_rf.pkl\"\n",
    "\n",
    "# RNN artifacts (từ notebook: tokenizer_rnn.pkl + *.keras)\n",
    "RNN_DIR = \"rnn_lstm_models\"  \n",
    "TOKENIZER_PKL   = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\tokenizer_lstm1d.pkl\"\n",
    "RNN_MODEL_KERAS = r\"C:\\Users\\PC\\CS221\\output_RNN_LSTM\\LSTM1D_model\\lstm1d_hate_speech.keras\"\n",
    "\n",
    "# BERT artifacts (từ notebook: checkpoints/bert_best)\n",
    "BERT_DIR = r\"C:\\Users\\PC\\CS221\\outputs_bert\\bert_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(x: str) -> str:\n",
    "    x = str(x).strip().lower()\n",
    "    if x in [\"hate\", \"hate_speech\", \"hate speech\", \"hatespeech\"]:\n",
    "        return \"Hate\"\n",
    "    if x in [\"offensive\", \"offensive_language\", \"offensive language\", \"offensivelanguage\"]:\n",
    "        return \"Offensive\"\n",
    "    if x in [\"neither\", \"neutral\", \"none\", \"clean\"]:\n",
    "        return \"Neither\"\n",
    "    return str(x)\n",
    "\n",
    "ID2LABEL_ML = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ml(texts, pipe_path=PIPE_ML_PKL):\n",
    "    \"\"\"\n",
    "    Load 1 sklearn Pipeline (.pkl) đã chứa tfidf + classifier.\n",
    "    Return:\n",
    "      - preds: list[str] (Hate/Offensive/Neither hoặc label gốc của model)\n",
    "      - proba: np.ndarray shape (n,3) hoặc None nếu model không hỗ trợ\n",
    "    \"\"\"\n",
    "    pipe = joblib.load(pipe_path)\n",
    "\n",
    "    # sklearn pipeline có thể predict ra string labels luôn (vì bạn train y là \"Hate\"/\"Offensive\"/\"neither\")\n",
    "    y_pred = pipe.predict(texts)\n",
    "\n",
    "    # normalize về chuẩn bạn dùng trong report\n",
    "    preds = [normalize_label(x) for x in y_pred]\n",
    "\n",
    "    proba = None\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        proba = pipe.predict_proba(texts)\n",
    "    return preds, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "def predict_bert(texts, batch_size=32, max_len=256):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_DIR)\n",
    "    model = BertForSequenceClassification.from_pretrained(BERT_DIR)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_probs = [], []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            pred_ids = probs.argmax(axis=1)\n",
    "\n",
    "        # theo notebook BERT: label2id = {'hate_speech':0,'offensive_language':1,'neither':2}\n",
    "        ID2LABEL_BERT = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}\n",
    "        all_preds.extend([ID2LABEL_BERT[int(j)] for j in pred_ids])\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    return all_preds, np.vstack(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff462838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\miniconda3\\envs\\hate-bert-ig\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:413: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\PC\\CS221\\preds_3pipelines.csv\n",
      "\n",
      "=== ML ===\n",
      "Accuracy: 0.6667\n",
      "Macro-F1: 0.2592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.0714    0.1667    0.1000         6\n",
      "   Offensive     1.0000    0.0526    0.1000        19\n",
      "     Neither     0.7619    0.9275    0.8366        69\n",
      "\n",
      "   micro avg     0.6667    0.7021    0.6839        94\n",
      "   macro avg     0.6111    0.3823    0.3455        94\n",
      "weighted avg     0.7660    0.7021    0.6407        94\n",
      "\n",
      "\n",
      "=== RNN ===\n",
      "Accuracy: 0.6263\n",
      "Macro-F1: 0.2268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.0000    0.0000    0.0000         6\n",
      "   Offensive     0.2222    0.1053    0.1429        19\n",
      "     Neither     0.6818    0.8696    0.7643        69\n",
      "\n",
      "   micro avg     0.6263    0.6596    0.6425        94\n",
      "   macro avg     0.3013    0.3249    0.3024        94\n",
      "weighted avg     0.5454    0.6596    0.5899        94\n",
      "\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.6566\n",
      "Macro-F1: 0.3856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.3571    0.8333    0.5000         6\n",
      "   Offensive     0.2857    0.2105    0.2424        19\n",
      "     Neither     0.7887    0.8116    0.8000        69\n",
      "\n",
      "   micro avg     0.6566    0.6915    0.6736        94\n",
      "   macro avg     0.4772    0.6185    0.5141        94\n",
      "weighted avg     0.6595    0.6915    0.6681        94\n",
      "\n",
      "Saved: C:\\Users\\PC\\CS221\\disagree_cases.csv | n_disagree = 44\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RNN_PREDS_CSV = r\"C:\\Users\\PC\\CS221\\rnn_preds.csv\"   # file bạn đã export\n",
    "\n",
    "# 1) ML + BERT inference\n",
    "pred_ml, proba_ml     = predict_ml(texts)\n",
    "pred_bert, proba_bert = predict_bert(texts)\n",
    "\n",
    "# 2) Load RNN preds từ CSV rồi merge theo thứ tự dòng\n",
    "rnn_df = pd.read_csv(RNN_PREDS_CSV)\n",
    "\n",
    "# cố gắng đoán tên cột dự đoán trong file rnn_preds.csv\n",
    "cand_cols = [c for c in [\"pred_RNN\", \"rnn_pred\", \"pred\", \"prediction\", \"label\"] if c in rnn_df.columns]\n",
    "if len(cand_cols) == 0:\n",
    "    # fallback: lấy cột đầu tiên\n",
    "    rnn_col = rnn_df.columns[0]\n",
    "else:\n",
    "    rnn_col = cand_cols[0]\n",
    "\n",
    "pred_rnn = rnn_df[rnn_col].astype(str).tolist()\n",
    "\n",
    "# check số dòng khớp\n",
    "assert len(pred_rnn) == len(df), f\"RNN preds rows ({len(pred_rnn)}) != df rows ({len(df)})\"\n",
    "\n",
    "# 3) Build output table\n",
    "out = df.copy()\n",
    "out[\"pred_ML\"]   = pred_ml\n",
    "out[\"pred_RNN\"]  = pred_rnn\n",
    "out[\"pred_BERT\"] = pred_bert\n",
    "\n",
    "# normalize label thật nếu có\n",
    "if LABEL_COL and (LABEL_COL in out.columns):\n",
    "    out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 4) Save merged predictions\n",
    "out_path = r\"C:\\Users\\PC\\CS221\\preds_3pipelines.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# 5) Evaluation (nếu có y_true)\n",
    "if \"y_true\" in out.columns:\n",
    "    y_true = out[\"y_true\"].tolist()\n",
    "    labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "    def eval_one(name, y_pred):\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "        print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "        print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "    eval_one(\"ML\",   out[\"pred_ML\"].tolist())\n",
    "    eval_one(\"RNN\",  out[\"pred_RNN\"].tolist())\n",
    "    eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "\n",
    "# 6) Disagreement analysis (không cần y_true)\n",
    "out[\"all_agree\"] = (out[\"pred_ML\"] == out[\"pred_RNN\"]) & (out[\"pred_RNN\"] == out[\"pred_BERT\"])\n",
    "disagree = out[~out[\"all_agree\"]].copy()\n",
    "\n",
    "disagree_path = r\"C:\\Users\\PC\\CS221\\disagree_cases.csv\"\n",
    "disagree.to_csv(disagree_path, index=False)\n",
    "print(\"Saved:\", disagree_path, \"| n_disagree =\", len(disagree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b7768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['STT', 'Tweet', 'Final Votes', 'pred_ML', 'pred_RNN', 'pred_BERT', 'y_true', 'all_agree']\n",
      "\n",
      "=== BERT ===\n",
      "Accuracy: 0.6566\n",
      "Macro-F1: 0.3856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hate     0.3571    0.8333    0.5000         6\n",
      "   Offensive     0.2857    0.2105    0.2424        19\n",
      "     Neither     0.7887    0.8116    0.8000        69\n",
      "\n",
      "   micro avg     0.6566    0.6915    0.6736        94\n",
      "   macro avg     0.4772    0.6185    0.5141        94\n",
      "weighted avg     0.6595    0.6915    0.6681        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# 1) check cột label thật\n",
    "print(\"Columns:\", list(out.columns))\n",
    "assert LABEL_COL in out.columns, f\"File test không có cột label thật '{LABEL_COL}'\"\n",
    "assert \"pred_BERT\" in out.columns, \"Chưa có cột pred_BERT (chưa chạy predict_bert hoặc chưa merge)\"\n",
    "\n",
    "# 2) tạo y_true chuẩn\n",
    "out[\"y_true\"] = out[LABEL_COL].apply(normalize_label)\n",
    "\n",
    "# 3) eval riêng cho BERT (và in luôn ML/RNN nếu muốn)\n",
    "labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "y_true = out[\"y_true\"].tolist()\n",
    "\n",
    "def eval_one(name, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_true, y_pred), 4))\n",
    "    print(\"Macro-F1:\", round(f1_score(y_true, y_pred, average=\"macro\"), 4))\n",
    "    print(classification_report(y_true, y_pred, labels=labels, digits=4))\n",
    "\n",
    "eval_one(\"BERT\", out[\"pred_BERT\"].tolist())\n",
    "# eval_one(\"ML\", out[\"pred_ML\"].tolist())\n",
    "# eval_one(\"RNN\", out[\"pred_RNN\"].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hate-bert-ig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
