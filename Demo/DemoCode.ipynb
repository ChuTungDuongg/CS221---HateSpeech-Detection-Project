{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Chu·∫©n b·ªã model v√† config:"
      ],
      "metadata": {
        "id": "fLVha6bdneOt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d31ebb66"
      },
      "source": [
        "import html\n",
        "import re\n",
        "\n",
        "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "USER_RE = re.compile(r\"@(\\w+)\")\n",
        "HASHTAG_RE = re.compile(r\"#(\\w+)\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Mgfk6l2A8_py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e7eb96-c882-41d2-af64-42fc8ea8883d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/NLP/Final_Project/Demo/BertModel.zip\" -d /content/bert_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL2H5n3e9umm",
        "outputId": "41be2309-e2ee-44ea-82af-e9da12da8fbf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/NLP/Final_Project/Demo/BertModel.zip\n",
            "replace /content/bert_model/outputs_bert/bert_best/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch gradio"
      ],
      "metadata": {
        "id": "AZJWPm0V9wXk"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"/content/bert_model/outputs_bert/bert_best\"\n"
      ],
      "metadata": {
        "id": "Frowpl1P--Wl"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenize & model:\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_DIR\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3AaafsW_CGe",
        "outputId": "d894da32-e607-4cd8-eb3d-1943260a9e65"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: \"Hate Speech\",\n",
        "    1: \"Offensive\",\n",
        "    2: \"Neither\"\n",
        "}\n",
        "\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id"
      ],
      "metadata": {
        "id": "t0hwC5PtAnsu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label mapping:\n",
        "label_map = {\n",
        "    0: \"Hate Speech\",\n",
        "    1: \"Offensive\",\n",
        "    2: \"Neither\"\n",
        "}"
      ],
      "metadata": {
        "id": "R-iVfB_W_U_t"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ki·ªÉm tra label mapping:\n",
        "print(model.config.id2label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_qRuUNM_w4-",
        "outputId": "e6e3ed23-d6a5-4466-f62e-743b920c498d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Hate Speech', 1: 'Offensive', 2: 'Neither'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping m√†u s·∫Øc:\n",
        "LABEL_COLORS = {\n",
        "    \"Hate Speech\": \"#d32f2f\",   # ƒë·ªè ƒë·∫≠m\n",
        "    \"Offensive\": \"#f57c00\",     # cam\n",
        "    \"Neither\": \"#388e3c\"        # xanh l√°\n",
        "}"
      ],
      "metadata": {
        "id": "6gX7jLFgi4iU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac57a456"
      },
      "source": [
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Thay v√¨ x√≥a s·∫°ch, d√πng token ƒë·ªÉ gi·ªØ t√≠n hi·ªáu \"c√≥ URL/mention\"\n",
        "    text = URL_RE.sub(' [URL] ', text)\n",
        "    text = USER_RE.sub(' [USER] ', text)\n",
        "\n",
        "    # Gi·ªØ n·ªôi dung hashtag, ch·ªâ b·ªè d·∫•u #\n",
        "    text = HASHTAG_RE.sub(r' \\1 ', text)\n",
        "\n",
        "    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Ch·ªâ b·∫≠t n·∫øu b·∫°n d√πng model uncased (vd: bert-base-uncased)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. H√†m d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh:"
      ],
      "metadata": {
        "id": "CYU6vwcxnsyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# H√†m d·ª± ƒëo√°n:\n",
        "def predict_hate_speech(text):\n",
        "    if text.strip() == \"\":\n",
        "        return \"Please enter a sentence.\", \"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "    conf, pred_id = torch.max(probs, dim=1)\n",
        "\n",
        "    label = model.config.id2label[pred_id.item()]\n",
        "    confidence = conf.item()\n",
        "\n",
        "    return label, f\"{confidence:.4f}\""
      ],
      "metadata": {
        "id": "3S9Ga4Ea_cWF"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install captum"
      ],
      "metadata": {
        "id": "qotSWhmaATKm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. T√≠ch h·ª£p c∆° ch·∫ø gi·∫£i th√≠ch b·∫±ng ph√¢n b·ªë attention:"
      ],
      "metadata": {
        "id": "4hbZePC4nyJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T√≠ch h·ª£p c∆° ch·∫ø gi·∫£i th√≠ch b·∫±ng c√°ch highlight c√°c t·ª´ ch√∫ √Ω:\n",
        "import torch\n",
        "import numpy as np\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Explainability: Integrated Gradients (word/token highlight)\n",
        "def _merge_wordpieces(tokens, scores):\n",
        "    # Merge BERT wordpieces (##) into whole words by summing scores.\n",
        "    words = []\n",
        "    word_scores = []\n",
        "    cur = \"\"\n",
        "    cur_score = 0.0\n",
        "\n",
        "    for t, s in zip(tokens, scores):\n",
        "        if t in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "        if t.startswith(\"##\"):\n",
        "            cur += t[2:]\n",
        "            cur_score += float(s)\n",
        "        else:\n",
        "            if cur:\n",
        "                words.append(cur)\n",
        "                word_scores.append(cur_score)\n",
        "            cur = t\n",
        "            cur_score = float(s)\n",
        "    if cur:\n",
        "        words.append(cur)\n",
        "        word_scores.append(cur_score)\n",
        "    return words, np.array(word_scores, dtype=np.float32)\n",
        "\n",
        "def explain_ig(text, target_label=None, max_length=128, n_steps=50):\n",
        "    model.eval()\n",
        "\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attn_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    def forward_embeds(embeds):\n",
        "        out = model(inputs_embeds=embeds, attention_mask=attn_mask)\n",
        "        return out.logits\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        probs = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n",
        "        pred_id = int(probs.argmax())\n",
        "        pred_label = id2label[pred_id]\n",
        "\n",
        "    if target_label is None:\n",
        "        target = pred_id\n",
        "    else:\n",
        "        if isinstance(target_label, str):\n",
        "            inv = {v: k for k, v in id2label.items()}\n",
        "            target = inv[target_label]\n",
        "        else:\n",
        "            target = int(target_label)\n",
        "\n",
        "    embeddings = model.get_input_embeddings()\n",
        "    input_embeds = embeddings(input_ids)\n",
        "    baseline_ids = torch.full_like(input_ids, tokenizer.pad_token_id)\n",
        "    baseline_embeds = embeddings(baseline_ids)\n",
        "\n",
        "    ig = IntegratedGradients(forward_embeds)\n",
        "    attributions = ig.attribute(\n",
        "        inputs=input_embeds,\n",
        "        baselines=baseline_embeds,\n",
        "        target=target,\n",
        "        n_steps=n_steps\n",
        "    )\n",
        "\n",
        "    token_scores = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).detach().cpu().tolist())\n",
        "\n",
        "    words, word_scores = _merge_wordpieces(tokens, token_scores)\n",
        "\n",
        "    imp = np.abs(word_scores)\n",
        "    if imp.max() > 0:\n",
        "        imp = imp / imp.max()\n",
        "    return pred_id, pred_label, probs, words, imp\n",
        "\n",
        "def render_highlight_html(words, scores, max_words=80):\n",
        "    words = words[:max_words]\n",
        "    scores = scores[:max_words]\n",
        "    spans = []\n",
        "    for w, s in zip(words, scores):\n",
        "        s = float(max(0.0, min(1.0, s)))\n",
        "        spans.append(\n",
        "            f'<span style=\"background: rgba(255, 0, 0, {0.15 + 0.75*s}); padding:2px 4px; margin:1px; border-radius:4px; display:inline-block;\">{w}</span>'\n",
        "        )\n",
        "    return \"<div style='line-height: 2.0;'>\" + \" \".join(spans) + \"</div>\"\n",
        "\n",
        "def render_confidence_html(probs, id2label):\n",
        "    probs = np.array(probs)\n",
        "    best_idx = probs.argmax()\n",
        "\n",
        "    rows = []\n",
        "    for i, p in enumerate(probs):\n",
        "        label = id2label[i]\n",
        "        color = LABEL_COLORS.get(label, \"#000000\")\n",
        "\n",
        "        style = \"\"\n",
        "        if i == best_idx:\n",
        "            style = f\"font-weight:bold; background-color: {color}20;\"\n",
        "\n",
        "        rows.append(\n",
        "            f\"\"\"\n",
        "            <tr style=\"{style}\">\n",
        "                <td style=\"padding:6px 12px; color:{color};\">\n",
        "                    {label}\n",
        "                </td>\n",
        "                <td style=\"padding:6px 12px;\">\n",
        "                    {p:.4f}\n",
        "                </td>\n",
        "            </tr>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <table style=\"border-collapse: collapse; margin-top: 8px; width: 60%;\">\n",
        "        <tr>\n",
        "            <th style=\"text-align:left; padding:6px 12px; border-bottom:1px solid #ccc;\">\n",
        "                Label\n",
        "            </th>\n",
        "            <th style=\"text-align:left; padding:6px 12px; border-bottom:1px solid #ccc;\">\n",
        "                Confidence\n",
        "            </th>\n",
        "        </tr>\n",
        "        {''.join(rows)}\n",
        "    </table>\n",
        "    \"\"\"\n",
        "    return html"
      ],
      "metadata": {
        "id": "V6HWKZ0raOMT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Thi·∫øt l·∫≠p UI cho Demo web:"
      ],
      "metadata": {
        "id": "xuEhAxoRn62F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. V·∫Ω bar-chart cho ph√¢n b·ªë x√°c su·∫•t nh√£n:"
      ],
      "metadata": {
        "id": "VwXEQhdpn_GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V·∫Ω bar chart cho ph√¢n b·ªë x√°c su·∫•t nh√£n:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confidence_bar(probs, id2label):\n",
        "    labels = [id2label[i] for i in range(len(probs))]\n",
        "    colors = [LABEL_COLORS.get(l, \"#999999\") for l in labels]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    ax.bar(labels, probs, color=colors)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_ylabel(\"Confidence\")\n",
        "    ax.set_title(\"Confidence Distribution\")\n",
        "\n",
        "    for i, v in enumerate(probs):\n",
        "        ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\", fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "9u8K9X6njQk9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. T√≠ch h·ª£p CSS ƒë·ªÉ c√≥ UI ƒë·∫πp:"
      ],
      "metadata": {
        "id": "Whzw3SyBoE0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T√≠ch h·ª£p CSS ƒë·ªÉ c√≥ UI demo x·ªãn:\n",
        "CUSTOM_CSS = \"\"\"\n",
        "#predict-btn {\n",
        "    font-size: 18px;\n",
        "    font-weight: bold;\n",
        "    padding: 10px;\n",
        "}\n",
        "\n",
        ".label-box textarea {\n",
        "    font-size: 20px !important;\n",
        "    font-weight: bold !important;\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        ".conf-box textarea {\n",
        "    font-size: 16px !important;\n",
        "    text-align: center;\n",
        "}\n",
        "\n",
        ".section-title {\n",
        "    font-size: 22px;\n",
        "    font-weight: 700;\n",
        "    margin-bottom: 8px;\n",
        "}\n",
        "\n",
        ".sub-title {\n",
        "    font-size: 16px;\n",
        "    font-weight: 600;\n",
        "    color: #555;\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jPfxc4bxkuP7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H√†m predict v·ªõi c∆° ch·∫ø attention:\n",
        "def gradio_predict_with_explain(text):\n",
        "    if text.strip() == \"\":\n",
        "        return (\n",
        "            \"N/A\",\n",
        "            \"0.0000\",\n",
        "            \"<i>No confidence available</i>\",\n",
        "            None,\n",
        "            \"<i>No explanation</i>\"\n",
        "        )\n",
        "\n",
        "    # Apply cleaning to the input text\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    pred_id, pred_label, probs, words, imp = explain_ig(cleaned_text)\n",
        "\n",
        "    pred_conf = float(probs[pred_id])\n",
        "\n",
        "    confidence_html = render_confidence_html(probs, id2label)\n",
        "    bar_plot = plot_confidence_bar(probs, id2label)\n",
        "    highlight_html = render_highlight_html(words, imp)\n",
        "\n",
        "    return (\n",
        "        pred_label,\n",
        "        f\"{pred_conf:.4f}\",\n",
        "        confidence_html,\n",
        "        bar_plot,\n",
        "        highlight_html\n",
        "    )"
      ],
      "metadata": {
        "id": "eX55XkdiafFk"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Giao di·ªán Demo v·ªõi hai c·ªôt ch√≠nh:"
      ],
      "metadata": {
        "id": "ZiBMUG50oOKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(css=CUSTOM_CSS, title=\"Hate Speech Detection Demo\") as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <div class=\"section-title\">üß† Hate Speech Detection using BERT</div>\n",
        "        <div class=\"sub-title\">\n",
        "        Explainability with Integrated Gradients (Token-level Highlighting)\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        # ================= LEFT COLUMN =================\n",
        "        with gr.Column(scale=1):\n",
        "\n",
        "            input_text = gr.Textbox(\n",
        "                lines=4,\n",
        "                placeholder=\"Enter an English sentence...\",\n",
        "                label=\"Input Text\"\n",
        "            )\n",
        "\n",
        "            predict_btn = gr.Button(\n",
        "                \"üöÄ Predict\",\n",
        "                elem_id=\"predict-btn\"\n",
        "            )\n",
        "\n",
        "            output_label = gr.Textbox(\n",
        "                label=\"Predicted Label\",\n",
        "                elem_classes=\"label-box\"\n",
        "            )\n",
        "\n",
        "            output_conf = gr.Textbox(\n",
        "                label=\"Predicted Label Confidence\",\n",
        "                elem_classes=\"conf-box\"\n",
        "            )\n",
        "\n",
        "        # ================= RIGHT COLUMN =================\n",
        "        with gr.Column(scale=1.3):\n",
        "\n",
        "            gr.Markdown(\"<div class='section-title'>üìä Model Confidence</div>\")\n",
        "\n",
        "            output_all_conf = gr.HTML(label=\"Confidence for All Labels\")\n",
        "\n",
        "            output_bar = gr.Plot(label=\"Confidence Bar Chart\")\n",
        "\n",
        "            gr.Markdown(\"<div class='section-title'>üîç Explainability</div>\")\n",
        "\n",
        "            output_explain = gr.HTML(label=\"Token Importance Visualization\")\n",
        "\n",
        "    predict_btn.click(\n",
        "        fn=gradio_predict_with_explain,\n",
        "        inputs=input_text,\n",
        "        outputs=[\n",
        "            output_label,\n",
        "            output_conf,\n",
        "            output_all_conf,\n",
        "            output_bar,\n",
        "            output_explain\n",
        "        ]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "5ByrKYYS_epF",
        "outputId": "4cddfa38-3c71-4c2c-c2d4-981e8197a5b3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3285868730.py:3: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=CUSTOM_CSS, title=\"Hate Speech Detection Demo\") as demo:\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/layouts/column.py:59: UserWarning: 'scale' value should be an integer. Using 1.3 will cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ccca34f4f84bf43b4a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ccca34f4f84bf43b4a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ccca34f4f84bf43b4a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}