# Hate Speech Detection Project

This repository contains coursework for building hate speech detection models, including a fine-tuned BERT classifier and RNN/LSTM baselines. The included notebooks walk through loading a labeled dataset, preprocessing text, training, and evaluating the models.

## Repository contents
- `NoteBook_Bert/`
  - `Bert_pipeline.ipynb`: end-to-end notebook covering data preparation, tokenization, model fine-tuning, and evaluation.
  - `Bert_inference_with_highlight.ipynb`: inference notebook with attention-based highlighting to interpret model outputs.
- `NoteBook_ML/`
  - `ML_pipeline.ipynb`: baseline ML workflow for preprocessing, training, and evaluation.
- `NoteBook_RNN_LSTM/`
  - `RNN_LSTM.ipynb`: RNN/LSTM notebook for sequence modeling experiments.
- `Model_RNN_LSTM/`: saved RNN/LSTM model checkpoints (if generated by the notebook).
- `Output_RNN_LSTM/`: logs or artifacts produced by the RNN/LSTM workflow (if generated by the notebook).
- `outputs_bert/`: saved artifacts such as logs, metrics, or exported models from the BERT workflow (if generated by the notebooks).
- `outputs_ML/`: saved artifacts from the baseline ML workflow (if generated by the notebooks).
- `Inference_3_pipeline/`: notebooks/scripts for comparing or running multiple pipelines (if present).
- `preds_3pipelines.csv`, `rnn_preds.csv`, `disagree_cases.csv`: example prediction outputs and disagreement analysis generated by experiments (if present).

## Prerequisites
- Python 3.9+
- Jupyter Notebook or JupyterLab
- GPU is recommended for training; CPU works for experimentation but will be slower.

## Setup
1. (Recommended) Create and activate a virtual environment.
2. Install dependencies with pip:
   ```bash
   pip install torch transformers datasets scikit-learn jupyter
   ```
   Choose the appropriate `torch` build for your hardware (CPU or CUDA).

## Quickstart
1. Launch Jupyter from the repo root:
   ```bash
   jupyter notebook
   ```
2. For the BERT workflow, open `NoteBook_Bert/Bert_pipeline.ipynb`.
3. Update dataset paths and label mappings as needed, then run the notebook cells in order.

## Data expectations
- The notebook assumes a labeled dataset with at least a `text` column and a corresponding numeric or categorical `label`.
- Update any dataset paths in the notebook to point to your local files.
- If your labels are not already numeric, map them to integers before training (see the preprocessing section of the notebook for guidance).

## Running the notebook
1. Start Jupyter from the repository root:
   ```bash
   jupyter notebook
   ```
2. Open the notebook you want to run (for example, `NoteBook_Bert/Bert_pipeline.ipynb`) in your browser.
3. Run the cells in order, adjusting file paths, label mappings, and hyperparameters (e.g., batch size, learning rate, number of epochs) to fit your environment.

## Inference workflows
- After training, open `Bert_inference_with_highlight.ipynb` to load your saved model and score new inputs with interpretability.
- Supply the same tokenizer/model artifacts saved during training.

## Tips for reproducible runs
- Capture your environment with `pip freeze > requirements.txt` after installing dependencies.
- Keep output directories separate per experiment to avoid overwriting logs and checkpoints.

## Evaluation and experimentation
- The notebook reports common classification metrics such as accuracy, precision, recall, and F1-score.
- To iterate quickly, try training on a subset of the data or reducing the number of epochs.
- Save trained models and tokenizer artifacts to reuse them without retraining (see the final cells for an example workflow).

## Next steps
- Add dataset download/processing scripts for repeatable experiments.
- Log metrics to a tracking tool (e.g., TensorBoard) for easier comparison across runs.
- Export the fine-tuned model for downstream applications such as content moderation pipelines.
