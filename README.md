# Hate Speech Detection Project ðŸ›¡ï¸ðŸ“

This repository contains coursework for building hate speech detection models, including a fine-tuned BERT classifier and RNN/LSTM baselines. The included notebooks walk through loading a labeled dataset, preprocessing text, training, and evaluating the models.

## Project approach (3 techniques) ðŸ§­
We tackle hate speech detection with three complementary techniques so results can be compared side by side:
1. **Traditional ML pipeline (feature-based)** ðŸ“Š  
   - Clean and normalize text, then extract features (e.g., TF-IDF).  
   - Train a classic classifier (e.g., logistic regression or linear SVM).  
   - Fast to train and easy to interpret for a strong baseline.
2. **RNN/LSTM sequence model** ðŸ”  
   - Tokenize text into sequences and feed them into an RNN/LSTM.  
   - Captures word order and longer-term dependencies.  
   - Serves as a neural baseline beyond bag-of-words features.
3. **Fine-tuned BERT transformer** ðŸ¤–  
   - Use a pretrained BERT model and fine-tune on the labeled dataset.  
   - Leverages contextual embeddings for improved accuracy.  
   - Supports interpretability via attention-based highlighting.

## Repository contents ðŸ“¦
- `NoteBook_Bert/`
  - `Bert_pipeline.ipynb`: end-to-end notebook covering data preparation, tokenization, model fine-tuning, and evaluation.
  - `Bert_inference_with_highlight.ipynb`: inference notebook with attention-based highlighting to interpret model outputs.
- `NoteBook_ML/`
  - `ML_pipeline.ipynb`: baseline ML workflow for preprocessing, training, and evaluation.
- `NoteBook_RNN_LSTM/`
  - `RNN_LSTM.ipynb`: RNN/LSTM notebook for sequence modeling experiments.
- `Model_RNN_LSTM/`: saved RNN/LSTM model checkpoints (if generated by the notebook).
- `Output_RNN_LSTM/`: logs or artifacts produced by the RNN/LSTM workflow (if generated by the notebook).
- `outputs_bert/`: saved artifacts such as logs, metrics, or exported models from the BERT workflow (if generated by the notebooks).
- `outputs_ML/`: saved artifacts from the baseline ML workflow (if generated by the notebooks).
- `Inference_3_pipeline/`: notebooks/scripts for comparing or running multiple pipelines (if present).
- `preds_3pipelines.csv`, `rnn_preds.csv`, `disagree_cases.csv`: example prediction outputs and disagreement analysis generated by experiments (if present).

## Prerequisites âœ…
- Python 3.9+
- Jupyter Notebook or JupyterLab
- GPU is recommended for training; CPU works for experimentation but will be slower.

## Setup âš™ï¸
1. (Recommended) Create and activate a virtual environment.
2. Install dependencies with pip:
   ```bash
   pip install torch transformers datasets scikit-learn jupyter
   ```
   Choose the appropriate `torch` build for your hardware (CPU or CUDA).

## Quickstart ðŸš€
1. Launch Jupyter from the repo root:
   ```bash
   jupyter notebook
   ```
2. For the BERT workflow, open `NoteBook_Bert/Bert_pipeline.ipynb`.
3. Update dataset paths and label mappings as needed, then run the notebook cells in order.

## Data expectations ðŸ§¾
- The notebook assumes a labeled dataset with at least a `text` column and a corresponding numeric or categorical `label`.
- Update any dataset paths in the notebook to point to your local files.
- If your labels are not already numeric, map them to integers before training (see the preprocessing section of the notebook for guidance).

## Running the notebook â–¶ï¸
1. Start Jupyter from the repository root:
   ```bash
   jupyter notebook
   ```
2. Open the notebook you want to run (for example, `NoteBook_Bert/Bert_pipeline.ipynb`) in your browser.
3. Run the cells in order, adjusting file paths, label mappings, and hyperparameters (e.g., batch size, learning rate, number of epochs) to fit your environment.

## Inference workflows ðŸ”
- After training, open `Bert_inference_with_highlight.ipynb` to load your saved model and score new inputs with interpretability.
- Supply the same tokenizer/model artifacts saved during training.

## Tips for reproducible runs â™»ï¸
- Capture your environment with `pip freeze > requirements.txt` after installing dependencies.
- Keep output directories separate per experiment to avoid overwriting logs and checkpoints.

## Evaluation and experimentation ðŸ“ˆ
- The notebook reports common classification metrics such as accuracy, precision, recall, and F1-score.
- To iterate quickly, try training on a subset of the data or reducing the number of epochs.
- Save trained models and tokenizer artifacts to reuse them without retraining (see the final cells for an example workflow).

## Next steps ðŸ§ª
- Add dataset download/processing scripts for repeatable experiments.
- Log metrics to a tracking tool (e.g., TensorBoard) for easier comparison across runs.
- Export the fine-tuned model for downstream applications such as content moderation pipelines.
